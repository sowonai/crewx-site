---
slug: crewx-0.6.0-release
title: CrewX 0.6.0 릴리스 - API Provider 지원 추가
authors: [doha]
tags: [crewx, release, ai]
---

CrewX 0.6.0이 출시되었습니다! 이번 릴리스는 **API Provider 지원**을 추가하여 CrewX의 AI 제공자 생태계를 크게 확장했습니다. 이제 CLI 기반 제공자뿐만 아니라 **OpenAI, Anthropic, Ollama** 등의 API 기반 AI 제공자를 직접 사용할 수 있으며, OpenRouter도 OpenAI 호환 API로 지원됩니다.

<!--truncate-->

## 🎯 주요 신기능

### 1. BYOA (Bring Your Own API) - 당신의 API를 가져오세요

CrewX는 처음부터 **BYOA (Bring Your Own AI)** 철학을 가지고 있었습니다. CLI 제공자(Claude Code, Gemini Code Assist, GitHub Copilot)를 통해 기존 AI 구독을 활용할 수 있었죠. 이제 **API Provider**를 추가하면서 이 철학이 한 단계 더 진화했습니다.

**기존 BYOA (CLI 제공자):**
```yaml
agents:
  - id: "my_agent"
    provider: "cli/claude"    # Claude Code CLI 사용
```

**새로운 BYOA (API 제공자):**
```yaml
agents:
  - id: "my_agent"
    provider: "api/anthropic"  # Anthropic API 직접 사용
    inline:
      apiKey: "{{env.ANTHROPIC_API_KEY}}"
      model: "claude-3-7-sonnet-20250219"
```

이제 당신이 이미 보유한 AI API 구독을 CrewX에서 바로 활용할 수 있습니다. **추가 비용 없이, 당신의 API로.**

### 2. API 제공자 지원

CrewX 0.6.0은 다음 API 제공자를 지원합니다:

#### ✅ 현재 지원 (테스트 완료)

**`api/openai` - OpenAI & OpenRouter**
- GPT-4, GPT-4 Turbo, GPT-3.5 모델 지원
- OpenRouter 자동 감지 (baseURL에 openrouter.ai 포함 시)
- OpenAI API 키로 직접 사용

```yaml
# OpenAI 직접 사용
- id: "gpt_agent"
  provider: "api/openai"
  inline:
    apiKey: "{{env.OPENAI_API_KEY}}"
    model: "gpt-4-turbo-preview"

# OpenRouter 사용
- id: "openrouter_agent"
  provider: "api/openai"
  inline:
    baseURL: "https://openrouter.ai/api/v1"
    apiKey: "{{env.OPENROUTER_API_KEY}}"
    model: "anthropic/claude-3.5-sonnet"
```

**`api/anthropic` - Anthropic Claude API**
- Claude 3.7 Sonnet, Claude 3.5 Sonnet, Claude 3 Opus, Haiku 지원
- Anthropic API 키로 직접 사용

```yaml
- id: "claude_agent"
  provider: "api/anthropic"
  inline:
    apiKey: "{{env.ANTHROPIC_API_KEY}}"
    model: "claude-3-7-sonnet-20250219"
```

**`api/ollama` - Ollama (로컬 모델)**
- Llama, Mistral 등 로컬 오픈소스 모델
- 인터넷 연결 없이 로컬 실행
- 무료로 사용 가능

```yaml
- id: "ollama_agent"
  provider: "api/ollama"
  inline:
    baseURL: "http://localhost:11434"
    model: "llama3.2"
```

#### 🔮 향후 지원 예정

다음 제공자들이 향후 버전에서 추가될 예정입니다:
- **Google Gemini** (`api/google`) - Gemini Pro, Ultra 지원
- **AWS Bedrock** (`api/bedrock`) - Claude, Titan, Llama 등
- **LiteLLM** (`api/litellm`) - 100+ AI 제공자 통합 프록시

### 3. Tool Calling - AI가 직접 파일을 읽고 쓸 수 있습니다

API Provider는 **Tool Calling** 기능을 내장하고 있습니다. 이제 AI 에이전트가 다음 도구를 사용할 수 있습니다:

#### Query Mode (읽기 전용)
- **`read_file`**: 파일 읽기
- **`grep`**: 패턴 검색
- **`ls`**: 디렉토리 목록

#### Execute Mode (쓰기 가능)
- **`write_file`**: 파일 쓰기
- **`replace`**: 텍스트 교체
- **`run_shell_command`**: 셸 명령 실행

**동작 방식:**
1. 사용자가 "README.md 파일을 읽어줘"라고 요청
2. AI가 `read_file` 도구를 호출하여 파일 내용 가져옴
3. AI가 파일 내용을 분석하여 응답

**Query vs Execute 모드:**
- **Query Mode**: 읽기 전용 도구만 사용 가능 (안전)
- **Execute Mode**: 파일 수정 및 셸 명령 실행 가능 (주의 필요)

```bash
# Query 모드 - 읽기만 가능
crewx query "@my_agent README.md 파일 분석해줘"

# Execute 모드 - 파일 수정 가능
crewx execute "@my_agent README.md에 설치 섹션 추가해줘"
```

### 4. 런타임 모델 오버라이드

에이전트 설정에서 기본 모델을 지정하되, 필요할 때 다른 모델을 사용할 수 있습니다:

```yaml
agents:
  - id: "smart_agent"
    provider: "api/anthropic"
    inline:
      apiKey: "{{env.ANTHROPIC_API_KEY}}"
      model: "claude-3-5-sonnet-20241022"  # 기본 모델
```

```bash
# 기본 모델 사용
crewx q "@smart_agent 간단한 질문"

# Opus 모델로 오버라이드 (복잡한 작업)
crewx q "@smart_agent:claude-3-opus-20240229 복잡한 아키텍처 설계해줘"

# Haiku 모델로 오버라이드 (빠른 응답)
crewx q "@smart_agent:claude-3-haiku-20240307 빠른 질문"
```

## 📋 CLI Provider vs API Provider 비교

| 특징 | CLI Provider | API Provider |
|------|-------------|--------------|
| **설정** | CLI 도구 설치 필요 | API 키만 필요 |
| **인증** | CLI 도구가 관리 | 직접 API 키 제공 |
| **모델** | CLI 도구에서 지정 | `agents.yaml`에서 지정 |
| **Tool Calling** | CLI 도구 의존 | CrewX 내장 도구 사용 |
| **비용** | CLI 구독 필요 | API 사용량 기반 |
| **네트워크** | CLI 도구 → API | 직접 API 호출 |
| **유연성** | CLI 도구 기능에 제한 | 모든 API 기능 활용 |

### 언제 무엇을 사용할까?

**CLI Provider 추천:**
- ✅ Claude Code, Gemini Code Assist, GitHub Copilot을 이미 사용 중
- ✅ IDE 통합 및 CLI 도구의 추가 기능이 필요
- ✅ 인증 관리를 CLI 도구에 위임하고 싶음

**API Provider 추천:**
- ✅ API 키를 직접 관리하고 싶음
- ✅ 다양한 모델을 자유롭게 전환하고 싶음
- ✅ Ollama 같은 로컬 모델 사용
- ✅ LiteLLM으로 여러 제공자를 통합 관리
- ✅ Tool Calling 기능을 최대한 활용하고 싶음

## 🚀 빠른 시작 예제

### 예제 1: Anthropic API로 코드 리뷰 에이전트

```yaml
agents:
  - id: "code_reviewer"
    provider: "api/anthropic"
    inline:
      apiKey: "{{env.ANTHROPIC_API_KEY}}"
      model: "claude-3-7-sonnet-20250219"
      prompt: |
        당신은 전문 코드 리뷰어입니다.
        코드의 버그, 성능 문제, 보안 취약점을 찾아내고
        개선 방안을 제시합니다.
```

```bash
# 파일 리뷰 (Query 모드 - 읽기만)
crewx q "@code_reviewer src/app.ts 파일 리뷰해줘"

# 수정까지 수행 (Execute 모드)
crewx x "@code_reviewer src/app.ts의 보안 문제 수정해줘"
```

### 예제 2: OpenAI + Ollama 멀티 에이전트

```yaml
agents:
  - id: "gpt_architect"
    provider: "api/openai"
    inline:
      apiKey: "{{env.OPENAI_API_KEY}}"
      model: "gpt-4-turbo-preview"
      prompt: |
        당신은 시스템 아키텍트입니다.

  - id: "local_coder"
    provider: "api/ollama"
    inline:
      baseURL: "http://localhost:11434"
      model: "llama3.2"
      prompt: |
        당신은 코드 구현 전문가입니다.
```

```bash
# 아키텍처 설계는 GPT-4로
crewx q "@gpt_architect 마이크로서비스 아키텍처 설계해줘"

# 코드 작성은 로컬 Llama로 (비용 절감)
crewx x "@local_coder API 엔드포인트 구현해줘"

# 동시에 두 에이전트에게 질문
crewx q "@gpt_architect @local_coder 이 설계의 장단점 분석해줘"
```

### 예제 3: LiteLLM으로 여러 제공자 통합

```yaml
agents:
  - id: "litellm_agent"
    provider: "api/litellm"
    inline:
      baseURL: "http://localhost:4000"
      apiKey: "{{env.LITELLM_API_KEY}}"
      model: "gpt-4"  # LiteLLM에서 라우팅
```

```bash
# LiteLLM 서버 시작
litellm --config litellm_config.yaml

# CrewX에서 사용
crewx q "@litellm_agent 질문해줘"
```

## 📦 추가 개선사항

- **환경 변수 치환**: `{{env.VAR}}` 문법으로 안전한 API 키 관리
- **인라인 설정 우선순위**: Root 레벨 설정을 에이전트별로 오버라이드 가능
- **에이전트 ID 추출**: API 에이전트를 YAML에서 정확하게 파싱
- **병렬 처리 개선**: 멀티 에이전트 실행 시 제공자 포맷 정규화

## 🔧 업그레이드 방법

```bash
# NPM으로 업데이트
npm update -g crewx

# 또는 재설치
npm install -g crewx@latest

# 버전 확인
crewx --version  # 0.6.0이 출력되어야 합니다
```

기존 `crewx.yaml` 설정은 그대로 동작합니다. API Provider를 사용하려면 위 예제처럼 `provider: "api/..."` 형식으로 에이전트를 추가하면 됩니다.

## 📚 다음 단계

CrewX 0.6.0으로 업그레이드하고 API Provider를 사용해보세요!

```bash
# 1. API 키 설정
export ANTHROPIC_API_KEY="your-key"
export OPENAI_API_KEY="your-key"

# 2. crewx.yaml 설정
vim crewx.yaml

# 3. 에이전트 실행
crewx q "@my_agent 안녕하세요!"
```

궁금한 점이 있으시면 [GitHub Issues](https://github.com/sowonlabs/crewx/issues)에 남겨주세요! 🙌
